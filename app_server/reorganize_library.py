import os
import json
import pyodbc
import re
import time
import google.generativeai as genai
from dotenv import load_dotenv

# --- C·∫§U H√åNH ---
load_dotenv()
API_KEY = "AIzaSyBLi_xp5bSdRXC8jpveV_mgumrushjZqBA" # Thay b·∫±ng Key th·∫≠t c·ªßa b·∫°n

db_server = os.getenv('DB_SERVER')
db_name = os.getenv('DB_NAME')
db_uid = os.getenv('DB_UID')
db_pwd = os.getenv('DB_PWD')

CONN_STR = (
    r'DRIVER={ODBC Driver 17 for SQL Server};'
    f'SERVER={db_server};DATABASE={db_name};UID={db_uid};PWD={db_pwd}'
)

genai.configure(api_key=API_KEY)
model = genai.GenerativeModel('gemini-2.5-flash')

def get_db_connection():
    return pyodbc.connect(CONN_STR)

def fetch_all_processed_materials(cursor):
    cursor.execute("SELECT MaterialID, FileName, Summary FROM TRAINING_MATERIALS WHERE AI_Processed = 1")
    rows = cursor.fetchall()
    materials_for_ai = []
    summary_map = {} 
    for row in rows:
        mat_id = row.MaterialID
        desc_text = ""
        if row.Summary:
            try:
                parsed = json.loads(row.Summary)
                desc_text = parsed.get('summary', row.Summary)
            except:
                desc_text = row.Summary
        materials_for_ai.append({
            "id": mat_id,
            "filename": row.FileName,
            "description": desc_text[:300] 
        })
        summary_map[mat_id] = row.Summary
    return materials_for_ai, summary_map

def ai_process_batch(batch_materials, existing_courses_info):
    """X·ª≠ l√Ω t·ª´ng m·∫ª nh·ªè, √©p AI gom nh√≥m quy·∫øt li·ªát v√†o existing_courses"""
    
    prompt = f"""
    B·∫°n l√† Gi√°m ƒë·ªëc ƒê√†o t·∫°o STD&D. D∆∞·ªõi ƒë√¢y l√† M·ªòT PH·∫¶N t√†i li·ªáu ƒë√†o t·∫°o (Batch) c·∫ßn ph√¢n lo·∫°i.
    
    M·ª§C TI√äU T·ªêI TH∆Ø·ª¢NG: GOM NH√ìM QUY·∫æT LI·ªÜT ƒê·ªÇ GI·∫¢M S·ªê L∆Ø·ª¢NG KH√ìA H·ªåC (T·ªîNG TO√ÄN H·ªÜ TH·ªêNG KH√îNG QU√Å 70 KH√ìA).
    Thay v√¨ chia nh·ªè l·∫Øt nh·∫Øt, h√£y t·∫°o ra c√°c Kh√≥a h·ªçc mang t√≠nh T·ªîNG H·ª¢P CAO (V√≠ d·ª•: Gom 'K·ªπ nƒÉng giao ti·∫øp', 'Email', 'Thuy·∫øt tr√¨nh' v√†o chung 1 kh√≥a 'K·ªπ nƒÉng l√†m vi·ªác chuy√™n nghi·ªáp'). M·ªói kh√≥a h·ªçc n√™n ch·ª©a t·ª´ 7 ƒë·∫øn 20 t√†i li·ªáu.

    NHI·ªÜM V·ª§ C·ª¶A B·∫†N:
    1. CATALOGUE & TR√ôNG L·∫∂P: ƒê∆∞a c√°c t√†i li·ªáu thu·∫ßn th√¥ng s·ªë, b·∫£n v·∫Ω, b·∫£ng gi√° v√†o 'catalogues'. ƒê∆∞a b·∫£n copy th·ª´a v√†o 'duplicates'.
    2. G·∫ÆN V√ÄO KH√ìA H·ªåC: 
       - B·∫†N B·∫ÆT BU·ªòC PH·∫¢I ∆ØU TI√äN T·ªêI ƒêA vi·ªác ƒë∆∞a t√†i li·ªáu v√†o c√°c kh√≥a h·ªçc ƒê√É T·∫†O ·ªü m·∫ª tr∆∞·ªõc (ƒê·ªçc k·ªπ M√¥ t·∫£ c·ªßa ch√∫ng trong danh s√°ch TR√ç NH·ªö b√™n d∆∞·ªõi).
       - CH·ªà ƒê∆Ø·ª¢C T·∫†O KH√ìA H·ªåC M·ªöI khi t√†i li·ªáu c√≥ ch·ªß ƒë·ªÅ ho√†n to√†n kh√°c bi·ªát v√† kh√¥ng th·ªÉ gh√©p chung v·ªõi b·∫•t k·ª≥ kh√≥a n√†o c≈©.
    
    8 CATEGORY CHU·∫®N ƒê∆Ø·ª¢C PH√âP D√ôNG:
    1. Ki·∫øn th·ª©c S·∫£n ph·∫©m
    2. Gi·ªõi thi·ªáu v·ªÅ STDD v√† nƒÉng l·ª±c cung c·∫•p
    3. K·ªπ nƒÉng m·ªÅm
    4. Quy tr√¨nh & V·∫≠n h√†nh
    5. Catalogue / Tra c·ª©u
    6. Quy ƒë·ªãnh & ch√≠nh s√°ch
    7. C√¥ng c·ª• & bi·ªÉu m·∫´u
    8. VƒÉn h√≥a & ph√°t tri·ªÉn c√° nh√¢n

    [TR√ç NH·ªö C·ª¶A B·∫†N] DANH S√ÅCH C√ÅC KH√ìA H·ªåC ƒê√É C√ì (H√ÉY T√åM M·ªåI C√ÅCH ƒê·ªÇ NH√âT T√ÄI LI·ªÜU V√ÄO ƒê√ÇY TR∆Ø·ªöC):
    {json.dumps(existing_courses_info, ensure_ascii=False)}

    [D·ªÆ LI·ªÜU C·∫¶N X·ª¨ L√ù L·∫¶N N√ÄY]:
    {json.dumps(batch_materials, ensure_ascii=False)}
    
    OUTPUT JSON B·∫ÆT BU·ªòC:
    {{
        "duplicates": [1, 2],
        "catalogues": [3, 4],
        "assignments": [
            {{
                "material_id": 5,
                "course_title": "T√™n Kh√≥a H·ªçc (C·ªë g·∫Øng copy y nguy√™n T√™n Kh√≥a h·ªçc t·ª´ TR√ç NH·ªö n·∫øu t√°i s·ª≠ d·ª•ng. N·∫øu b·∫Øt bu·ªôc t·∫°o m·ªõi th√¨ vi·∫øt t√™n bao qu√°t, t·ªïng h·ª£p)",
                "course_desc": "Ch·ªâ vi·∫øt m√¥ t·∫£ n·∫øu ƒë√¢y l√† Kh√≥a h·ªçc T·∫†O M·ªöI. N·∫øu x√†i kh√≥a c≈© th√¨ ƒë·ªÉ r·ªóng.",
                "category": "T√™n 1 trong 8 Category",
                "sub_category": "T√™n SubCategory"
            }}
        ]
    }}
    """
    
    try:
        response = model.generate_content(
            prompt,
            generation_config=genai.GenerationConfig(
                response_mime_type="application/json"
            )
        )
        return json.loads(response.text)
    except Exception as e:
        print(f"   ‚ùå L·ªói g·ªçi AI: {e}")
        return None

def main():
    conn = get_db_connection()
    cursor = conn.cursor()
    
    print("üìö B∆Ø·ªöC 1: ƒê·ªçc d·ªØ li·ªáu t√†i li·ªáu hi·ªán c√≥...")
    materials, summary_map = fetch_all_processed_materials(cursor)
    print(f"-> L·∫•y th√†nh c√¥ng {len(materials)} t√†i li·ªáu.")
    if not materials: return

    BATCH_SIZE = 50
    batches = [materials[i:i + BATCH_SIZE] for i in range(0, len(materials), BATCH_SIZE)]
    print(f"-> ƒê√£ chia th√†nh {len(batches)} m·∫ª ƒë·ªÉ qu√©t an to√†n.")

    global_duplicates = set()
    global_catalogues = set()
    global_courses = {} 

    print("\nüß† B∆Ø·ªöC 2: TI·∫æN H√ÄNH QU√âT V√Ä GOM NH√ìM QUY·∫æT LI·ªÜT...")
    
    for idx, batch in enumerate(batches):
        print(f"\n‚è≥ ƒêang x·ª≠ l√Ω m·∫ª {idx+1}/{len(batches)}...")
        
        # [C·∫¢I TI·∫æN QUAN TR·ªåNG] G·ª≠i to√†n b·ªô Info (T√™n + M√¥ t·∫£) c·ªßa kh√≥a h·ªçc c≈© cho AI
        existing_courses_info = [
            {"title": title, "desc": info["desc"], "category": info["category"]} 
            for title, info in global_courses.items()
        ]
        
        result = ai_process_batch(batch, existing_courses_info)
        if not result:
            print(f"   ‚ö†Ô∏è M·∫ª {idx+1} th·∫•t b·∫°i. B·ªè qua m·∫ª n√†y.")
            continue
            
        global_duplicates.update(result.get('duplicates', []))
        global_catalogues.update(result.get('catalogues', []))
        
        assignments = result.get('assignments', [])
        for assign in assignments:
            mat_id = assign.get('material_id')
            title = assign.get('course_title', '').strip()
            if not mat_id or not title: continue
            
            # G√°n v√†o kh√≥a c≈© ho·∫∑c ƒë·∫ª kh√≥a m·ªõi
            if title not in global_courses:
                global_courses[title] = {
                    "desc": assign.get('course_desc', f"Kh√≥a h·ªçc chuy√™n s√¢u v·ªÅ {title}"),
                    "category": assign.get('category', 'Kh√°c'),
                    "sub_category": assign.get('sub_category', 'Ch∆∞a ph√¢n lo·∫°i'),
                    "materials": []
                }
                
            global_courses[title]["materials"].append(mat_id)
            
            old_sum = summary_map.get(mat_id)
            if old_sum:
                try:
                    s_dict = json.loads(old_sum)
                    s_dict['category'] = assign.get('category')
                    s_dict['sub_category'] = assign.get('sub_category')
                    summary_map[mat_id] = json.dumps(s_dict, ensure_ascii=False)
                except: pass

        print(f"   -> H·ªá th·ªëng hi·ªán ƒëang ghi nh·∫≠n t·ªïng c·ªông {len(global_courses)} Kh√≥a h·ªçc l·ªõn.")
        time.sleep(3) 

    print(f"\n======================================")
    print(f"-> T·ªîNG K·∫æT: ƒê√£ c√¥ ƒë·ªçng th√†nh {len(global_courses)} Kh√≥a h·ªçc (Gi·∫£m thi·ªÉu ph√¢n m·∫£nh).")
    
    print("\n‚öôÔ∏è B∆Ø·ªöC 3: C·∫¨P NH·∫¨T DATABASE CH√çNH TH·ª®C...")
    try:
        cursor.execute("UPDATE TRAINING_MATERIALS SET CourseID = NULL")
        
        if global_catalogues:
            print("   - ƒêang ƒë·∫©y t√†i li·ªáu v√†o kho Catalogue...")
            for cat_id in global_catalogues:
                old_sum = summary_map.get(cat_id)
                if old_sum:
                    try:
                        s_dict = json.loads(old_sum)
                        s_dict['category'] = "Catalogue / Tra c·ª©u"
                        s_dict['sub_category'] = "T√†i li·ªáu k·ªπ thu·∫≠t"
                        cursor.execute("UPDATE TRAINING_MATERIALS SET Summary = ? WHERE MaterialID = ?", (json.dumps(s_dict, ensure_ascii=False), cat_id))
                    except: pass

        cursor.execute("DELETE FROM TRAINING_COURSES")
        
        print("   - ƒêang thi·∫øt l·∫≠p c·∫•u tr√∫c Kh√≥a h·ªçc m·ªõi...")
        for title, c_data in global_courses.items():
            valid_ids = [i for i in c_data['materials'] if i not in global_duplicates and i not in global_catalogues]
            if not valid_ids: continue
            
            cursor.execute("""
                INSERT INTO TRAINING_COURSES (Title, Description, Category, SubCategory, ThumbnailUrl, IsMandatory, CreatedDate, XP_Reward)
                OUTPUT INSERTED.CourseID
                VALUES (?, ?, ?, ?, '/static/img/3d_assets/culture/books.png', 0, GETDATE(), 300)
            """, (title, c_data['desc'], c_data['category'], c_data['sub_category']))
            
            new_course_id = cursor.fetchone()[0]
            
            placeholders = ','.join('?' * len(valid_ids))
            sql_up = f"UPDATE TRAINING_MATERIALS SET CourseID = ? WHERE MaterialID IN ({placeholders})"
            cursor.execute(sql_up, [new_course_id] + valid_ids)

        for mat_id, sum_json in summary_map.items():
            cursor.execute("UPDATE TRAINING_MATERIALS SET Summary = ? WHERE MaterialID = ?", (sum_json, mat_id))

        conn.commit()
        print("\nüéâ TH√ÄNH C√îNG! Th∆∞ vi·ªán ƒë√£ ƒë∆∞·ª£c quy ho·∫°ch g·ªçn g√†ng, s√∫c t√≠ch.")
        
    except Exception as e:
        conn.rollback()
        print(f"\n‚ùå L·ªñI DATABASE: ƒê√£ rollback. Chi ti·∫øt: {e}")
    finally:
        conn.close()

if __name__ == "__main__":
    main()