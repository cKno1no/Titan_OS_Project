import os
import time
import json
import pyodbc
import PyPDF2
import re
import google.generativeai as genai
from dotenv import load_dotenv
from collections import Counter

# --- C·∫§U H√åNH ---
load_dotenv()
API_KEY = "AIzaSyBLi_xp5bSdRXC8jpveV_mgumrushjZqBA" # Ho·∫∑c l·∫•y t·ª´ env

db_server = os.getenv('DB_SERVER')
db_name = os.getenv('DB_NAME')
db_uid = os.getenv('DB_UID')
db_pwd = os.getenv('DB_PWD')

CONN_STR = (
    r'DRIVER={ODBC Driver 17 for SQL Server};'
    f'SERVER={db_server};'
    f'DATABASE={db_name};'
    f'UID={db_uid};PWD={db_pwd}'
)



genai.configure(api_key=API_KEY)
model = genai.GenerativeModel('gemini-2.5-flash')

def get_db_connection():
    return pyodbc.connect(CONN_STR)

def clean_json_string(text):
    text = re.sub(r"```json|```", "", text).strip()
    s = text.find('[')
    e = text.rfind(']')
    if s != -1 and e != -1:
        return text[s:e+1]
    return "[]"

# 1. H√ÄM ƒê·ªåC PDF
def extract_text_smart(pdf_path, max_pages=15):
    try:
        real_path = pdf_path.lstrip('/') if pdf_path.startswith('/') else pdf_path
        real_path = real_path.replace('/', os.sep)
        if not os.path.exists(real_path): return ""
        text = ""
        with open(real_path, 'rb') as f:
            reader = PyPDF2.PdfReader(f)
            for i in range(min(len(reader.pages), max_pages)):
                page_text = reader.pages[i].extract_text()
                if page_text: text += page_text + "\n"
        return text
    except: return ""

# ==============================================================================
# 2. TASK A: T·∫†O 7 C√ÇU H·ªéI M·ªöI (5 TH∆Ø·ªúNG - 2 KH√ì)
# ==============================================================================
def task_generate_new_questions(material_title, full_text):
    print(f"   [Task A] ƒêang s√°ng t·∫°o 7 c√¢u h·ªèi m·ªõi (5 Th∆∞·ªùng, 2 Kh√≥)...")
    
    # L·∫•y 15000 k√Ω t·ª± ƒë·∫°i di·ªán
    context = full_text[:15000]
    
    prompt = f"""
    T√†i li·ªáu: "{material_title}"
    N·ªôi dung tr√≠ch d·∫´n:
    {context}
    ...
    
    NHI·ªÜM V·ª§: T·∫°o ƒë√∫ng 07 c√¢u h·ªèi tr·∫Øc nghi·ªám (4 ƒë√°p √°n) ƒë·ªÉ ki·ªÉm tra ng∆∞·ªùi h·ªçc.
    
    C·∫§U TR√öC B·∫ÆT BU·ªòC:
    1. **05 C√¢u M·ª©c ƒë·ªô Th√¥ng hi·ªÉu (Normal):** Ki·ªÉm tra ki·∫øn th·ª©c c∆° b·∫£n trong b√†i.
    2. **02 C√¢u M·ª©c ƒë·ªô V·∫≠n d·ª•ng (Hard):** C√¢u h·ªèi t√¨nh hu·ªëng ho·∫∑c suy lu·∫≠n, ƒë√≤i h·ªèi hi·ªÉu s√¢u m·ªõi l√†m ƒë∆∞·ª£c.
    
    Y√äU C·∫¶U:
    - ƒê√°p √°n ph·∫£i n·∫±m trong n·ªôi dung t√†i li·ªáu.
    - Gi·∫£i th√≠ch (explain) ng·∫Øn g·ªçn t·∫°i sao ƒë√∫ng.
    
    OUTPUT JSON:
    [
        {{ 
            "content": "C√¢u h·ªèi...", 
            "a": "...", "b": "...", "c": "...", "d": "...", 
            "correct": "A", 
            "explain": "...", 
            "difficulty": "Hard" (ho·∫∑c "Normal") 
        }}
    ]
    """
    for attempt in range(3):
        try:
            response = model.generate_content(prompt)
            json_str = clean_json_string(response.text)
            data = json.loads(json_str)
            # Validate s·ªë l∆∞·ª£ng n·∫øu c·∫ßn, nh∆∞ng AI th∆∞·ªùng l√†m ƒë√∫ng
            if data: return data
        except Exception as e:
            print(f"      ‚ö†Ô∏è L·ªói sinh c√¢u h·ªèi (L·∫ßn {attempt+1}): {e}")
            time.sleep(2)
    return []

# ==============================================================================
# H√ÄM PH·ª§ TR·ª¢: CH·∫§M ƒêI·ªÇM KH·ªöP T·ª™ KH√ìA (SCORING)
# ==============================================================================
def calculate_relevance_score(question_row, material_text_lower):
    """
    T√≠nh ƒëi·ªÉm ƒë·ªô ph√π h·ª£p c·ªßa c√¢u h·ªèi v·ªõi b√†i h·ªçc.
    Score = S·ªë l·∫ßn c√°c t·ª´ quan tr·ªçng trong C√¢u h·ªèi & ƒê√°p √°n xu·∫•t hi·ªán trong B√†i h·ªçc.
    """
    # 1. G·ªôp n·ªôi dung c√¢u h·ªèi v√† ƒë√°p √°n ƒë√∫ng
    content_to_check = f"{question_row.Content} {question_row.CorrectAnswer}"
    
    # 2. T√°ch t·ª´, l·ªçc b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát v√† t·ª´ ng·∫Øn
    words = re.findall(r'\w+', content_to_check.lower())
    significant_words = [w for w in words if len(w) > 3] # Ch·ªâ l·∫•y t·ª´ d√†i > 3 k√Ω t·ª±
    
    if not significant_words: return 0
    
    # 3. ƒê·∫øm s·ªë l·∫ßn xu·∫•t hi·ªán trong b√†i h·ªçc
    score = 0
    for word in significant_words:
        if word in material_text_lower:
            score += 1
            
    return score

# ==============================================================================
# 3. TASK B: MAP C√ÇU H·ªéI C≈® (C√ì SCORING)
# ==============================================================================
def task_map_with_scoring(cursor, material_id, summary, full_text):
    print(f"   [Task B] Qu√©t kho c≈© & Ch·∫•m ƒëi·ªÉm ph√π h·ª£p...")
    
    # 1. L·ªçc th√¥ b·∫±ng SQL (L·∫•y r·ªông ra kho·∫£ng 50-100 ·ª©ng vi√™n)
    try:
        keywords = summary.split()[:30] if summary else full_text[:500].split()
        keywords = [k for k in keywords if len(k) > 3][:10]
    except: keywords = []

    if not keywords: return 0

    conditions = []
    params = []
    for kw in keywords:
        conditions.append("Content LIKE ?")
        params.append(f"%{kw}%")
            
    if not conditions: return 0
    
    # L·∫•y TOP 100 c√¢u c√≥ ch·ª©a t·ª´ kh√≥a (L·∫•y d∆∞ ƒë·ªÉ ch·∫•m ƒëi·ªÉm l·∫°i)
    sql_filter = f"""
        SELECT TOP 100 ID, Content, CorrectAnswer 
        FROM TRAINING_QUESTION_BANK 
        WHERE SourceMaterialID IS NULL 
        AND ({' OR '.join(conditions)})
    """
    candidates = cursor.execute(sql_filter, tuple(params)).fetchall()
    
    if not candidates: return 0

    # 2. CH·∫§M ƒêI·ªÇM (SCORING) - Python Logic
    # Ch·ªâ gi·ªØ l·∫°i nh·ªØng c√¢u c√≥ Score cao (t·ª©c l√† n·ªôi dung c√¢u h·ªèi xu·∫•t hi·ªán nhi·ªÅu trong b√†i)
    scored_candidates = []
    material_lower = full_text.lower()
    
    for cand in candidates:
        score = calculate_relevance_score(cand, material_lower)
        # Ng∆∞·ª°ng l·ªçc: √çt nh·∫•t ph·∫£i kh·ªõp 2 t·ª´ kh√≥a quan tr·ªçng tr·ªü l√™n
        if score >= 2: 
            scored_candidates.append({'data': cand, 'score': score})
            
    # S·∫Øp x·∫øp theo ƒëi·ªÉm gi·∫£m d·∫ßn v√† l·∫•y TOP 20
    scored_candidates.sort(key=lambda x: x['score'], reverse=True)
    top_candidates = [x['data'] for x in scored_candidates[:20]]
    
    if not top_candidates:
        print("      -> Kh√¥ng c√≥ c√¢u h·ªèi n√†o ƒë·∫°t ƒëi·ªÉm ph√π h·ª£p.")
        return 0
        
    print(f"      -> ƒê√£ l·ªçc ƒë∆∞·ª£c {len(top_candidates)} c√¢u h·ªèi c√≥ ƒë·ªô kh·ªõp cao nh·∫•t (Score cao). G·ª≠i AI check...")

    # 3. G·ª≠i AI Check l·∫ßn cu·ªëi (Final Verification)
    short_context = full_text[:8000]
    candidate_list = [{"id": r.ID, "q": r.Content, "a": r.CorrectAnswer} for r in top_candidates]
    
    verify_prompt = f"""
    T√†i li·ªáu:
    {short_context}
    ...
    
    Danh s√°ch c√¢u h·ªèi & ƒë√°p √°n:
    {json.dumps(candidate_list, ensure_ascii=False)}
    
    NHI·ªÜM V·ª§ QUAN TR·ªåNG:
    Ch·ªâ ch·ªçn nh·ªØng c√¢u h·ªèi m√† **ƒê√°p √°n (a)** C√ì TH·ªÇ ƒê∆Ø·ª¢C T√åM TH·∫§Y ho·∫∑c SUY LU·∫¨N ƒê∆Ø·ª¢C t·ª´ T√†i li·ªáu tr√™n.
    N·∫øu t√†i li·ªáu kh√¥ng nh·∫Øc ƒë·∫øn ki·∫øn th·ª©c ƒë√≥, tuy·ªát ƒë·ªëi kh√¥ng ch·ªçn.
    
    OUTPUT JSON: [id1, id2...]
    """
    
    for attempt in range(3):
        try:
            res = model.generate_content(verify_prompt)
            json_str = clean_json_string(res.text)
            valid_ids = json.loads(json_str)
            
            valid_ids = [i for i in valid_ids if isinstance(i, int)]
            if valid_ids:
                placeholders = ','.join('?' * len(valid_ids))
                sql_update = f"UPDATE TRAINING_QUESTION_BANK SET SourceMaterialID = ? WHERE ID IN ({placeholders})"
                cursor.execute(sql_update, [material_id] + valid_ids)
                return len(valid_ids)
            return 0
        except Exception as e:
            print(f"      ‚ö†Ô∏è L·ªói AI Map: {e}")
            time.sleep(2)
    return 0

def main():
    conn = get_db_connection()
    cursor = conn.cursor()
    
    # L·∫•y t√†i li·ªáu ch∆∞a c√≥ c√¢u h·ªèi (ho·∫∑c ch·∫°y h·∫øt n·∫øu mu·ªën update)
    sql = """
        SELECT MaterialID, FileName, FilePath, Summary 
        FROM TRAINING_MATERIALS 
        WHERE MaterialID NOT IN (
            SELECT DISTINCT SourceMaterialID FROM TRAINING_QUESTION_BANK WHERE SourceMaterialID IS NOT NULL
        )
    """
    materials = cursor.execute(sql).fetchall()
    
    print(f"üöÄ B·∫Øt ƒë·∫ßu x·ª≠ l√Ω {len(materials)} t√†i li·ªáu (Quy tr√¨nh chu·∫©n)...")
    
    for idx, m in enumerate(materials):
        print(f"\n[{idx+1}/{len(materials)}] X·ª≠ l√Ω: {m.FileName}...")
        
        full_text = extract_text_smart(m.FilePath)
        if not full_text:
            print("   ‚ö†Ô∏è File r·ªóng. B·ªè qua.")
            continue

        # TASK A: T·∫°o 7 c√¢u (5 Th∆∞·ªùng, 2 Kh√≥)
        new_qs = task_generate_new_questions(m.FileName, full_text)
        if new_qs:
            for q in new_qs:
                diff = q.get('difficulty', 'Normal')
                cursor.execute("""
                    INSERT INTO TRAINING_QUESTION_BANK 
                    (Content, OptionA, OptionB, OptionC, OptionD, CorrectAnswer, Explanation, SourceMaterialID, Category, Difficulty, IsAI_Generated, CreatedDate)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, 'Assessment', ?, 1, GETDATE())
                """, (q['content'], q['a'], q['b'], q['c'], q['d'], q['correct'], q.get('explain',''), m.MaterialID, diff))
            print(f"   ‚úÖ ƒê√£ t·∫°o {len(new_qs)} c√¢u m·ªõi.")
            conn.commit()

        # TASK B: Map c√¢u c≈© (C√≥ Scoring)
        mapped = task_map_with_scoring(cursor, m.MaterialID, m.Summary, full_text)
        if mapped:
            print(f"   üîó ƒê√£ map th√™m {mapped} c√¢u c≈© ph√π h·ª£p.")
            conn.commit()
            
        time.sleep(1)

    print("\nüéâ HO√ÄN T·∫§T!")
    conn.close()

if __name__ == "__main__":
    main()