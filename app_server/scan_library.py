import os
import time
import json
import pyodbc
import PyPDF2
import google.generativeai as genai
from dotenv import load_dotenv

# --- C·∫§U H√åNH ---
load_dotenv()
API_KEY = "AIzaSyCC_qWqKqqupwwUT7mOR_Z75M9eKv8Vil4" # Ho·∫∑c l·∫•y t·ª´ env

db_server = os.getenv('DB_SERVER')
db_name = os.getenv('DB_NAME')
db_uid = os.getenv('DB_UID')
db_pwd = os.getenv('DB_PWD')

CONN_STR = (
    r'DRIVER={ODBC Driver 17 for SQL Server};'
    f'SERVER={db_server};'
    f'DATABASE={db_name};'
    f'UID={db_uid};PWD={db_pwd}'
)

# Th∆∞ m·ª•c ch·ª©a PDF
LIBRARY_DIR = r'static/uploads/library' 

genai.configure(api_key=API_KEY)
model = genai.GenerativeModel('gemini-2.5-flash')

def get_db_connection():
    return pyodbc.connect(CONN_STR)

def extract_text_from_pdf(pdf_path, max_pages=5):
    """ƒê·ªçc text t·ª´ PDF (ch·ªâ l·∫•y max_pages trang ƒë·∫ßu ƒë·ªÉ ti·∫øt ki·ªám token)"""
    text = ""
    try:
        with open(pdf_path, 'rb') as f:
            reader = PyPDF2.PdfReader(f)
            num_pages = len(reader.pages)
            for i in range(min(num_pages, max_pages)):
                page = reader.pages[i]
                text += page.extract_text() + "\n"
        return text, num_pages
    except Exception as e:
        print(f"‚ùå L·ªói ƒë·ªçc file {pdf_path}: {e}")
        return None, 0

def analyze_document_with_ai(filename, text_content):
    """G·ª≠i text l√™n AI ƒë·ªÉ l·∫•y Metadata"""
    prompt = f"""
    B·∫°n l√† qu·∫£n th∆∞ AI. H√£y ph√¢n t√≠ch t√†i li·ªáu c√≥ t√™n file: "{filename}" v√† n·ªôi dung ƒë·∫ßu sau:
    ---
    {text_content[:3000]}
    ---
    
    NHI·ªÜM V·ª§: Tr·∫£ v·ªÅ k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng JSON thu·∫ßn (kh√¥ng markdown) v·ªõi c√°c tr∆∞·ªùng sau:
    1. "title": Ti√™u ƒë·ªÅ t√†i li·ªáu chu·∫©n h√≥a (Ti·∫øng Vi·ªát, vi·∫øt hoa ch·ªØ c√°i ƒë·∫ßu, b·ªè ƒëu√¥i .pdf).
    2. "category": Ph√¢n lo·∫°i ch·ªß ƒë·ªÅ (VD: K·ªπ thu·∫≠t, B√°n h√†ng, Nh√¢n s·ª±, Ph√°p l√Ω, S·∫£n ph·∫©m).
    3. "summary": T√≥m t·∫Øt n·ªôi dung t√†i li·ªáu trong kho·∫£ng 3-4 d√≤ng s√∫c t√≠ch.
    4. "keywords": 5 t·ª´ kh√≥a ch√≠nh c√°ch nhau b·∫±ng d·∫•u ph·∫©y.
    """
    
    try:
        response = model.generate_content(prompt)
        cleaned_text = response.text.replace('```json', '').replace('```', '').strip()
        return json.loads(cleaned_text)
    except Exception as e:
        print(f"‚ùå L·ªói AI ph√¢n t√≠ch: {e}")
        return None

def main():
    print(f"--- B·∫ÆT ƒê·∫¶U QU√âT KHO T√ÄI LI·ªÜU T·∫†I: {LIBRARY_DIR} ---")
    
    if not os.path.exists(LIBRARY_DIR):
        print(f"Kh√¥ng t√¨m th·∫•y th∆∞ m·ª•c {LIBRARY_DIR}. H√£y t·∫°o n√≥ v√† copy file PDF v√†o.")
        return

    conn = get_db_connection()
    cursor = conn.cursor()
    
    # Duy·ªát qua t·∫•t c·∫£ c√°c file trong th∆∞ m·ª•c (bao g·ªìm th∆∞ m·ª•c con)
    count = 0
    for root, dirs, files in os.walk(LIBRARY_DIR):
        for file in files:
            if not file.lower().endswith('.pdf'): continue
            
            file_path = os.path.join(root, file)
            rel_path = os.path.relpath(file_path, start=os.getcwd()) # ƒê∆∞·ªùng d·∫´n t∆∞∆°ng ƒë·ªëi ƒë·ªÉ l∆∞u DB
            
            # 1. Ki·ªÉm tra xem file ƒë√£ c√≥ trong DB ch∆∞a (tr√°nh tr√πng l·∫∑p)
            cursor.execute("SELECT MaterialID FROM TRAINING_MATERIALS WHERE FileName = ?", (file,))
            if cursor.fetchone():
                print(f"‚è© B·ªè qua (ƒê√£ t·ªìn t·∫°i): {file}")
                continue

            print(f"\nüìÑ ƒêang x·ª≠ l√Ω: {file}...")
            
            # 2. ƒê·ªçc n·ªôi dung PDF
            pdf_text, total_pages = extract_text_from_pdf(file_path)
            if not pdf_text: continue

            # 3. G·ªçi AI ph√¢n t√≠ch
            print("   -> ƒêang g·ªçi Gemini AI ph√¢n t√≠ch...")
            ai_data = analyze_document_with_ai(file, pdf_text)
            
            if ai_data:
                # 4. L∆∞u v√†o Database
                # M·∫πo: T·ª± ƒë·ªông t·∫°o Course ·∫£o n·∫øu ch∆∞a c√≥, ho·∫∑c g√°n v√†o Course "General"
                # ·ªû ƒë√¢y t√¥i insert th·∫≥ng v√†o TRAINING_MATERIALS, s·∫øp c√≥ th·ªÉ map CourseID sau
                
                # L∆∞u file JSON n·ªôi dung ƒë·ªÉ Chatbot d√πng sau n√†y (Split View)
                json_content = [{"page": 1, "content": pdf_text}] # Demo l∆∞u trang 1, th·ª±c t·∫ø n√™n l∆∞u full
                with open(file_path + ".json", 'w', encoding='utf-8') as f:
                    json.dump(json_content, f, ensure_ascii=False)

                sql = """
                    INSERT INTO TRAINING_MATERIALS 
                    (FileName, FilePath, TotalPages, Summary, CreatedDate, AI_Processed, CourseID)
                    VALUES (?, ?, ?, ?, GETDATE(), 1, NULL) -- CourseID NULL ch·ªù admin x·∫øp l·ªõp sau
                """
                # T·∫°m th·ªùi l∆∞u Title v√† Category v√†o Summary ho·∫∑c t·∫°o c·ªôt m·ªõi n·∫øu s·∫øp mu·ªën
                # ·ªû ƒë√¢y t√¥i l∆∞u Title v√†o FileName hi·ªÉn th·ªã cho ƒë·∫πp
                final_summary = f"**Ch·ªß ƒë·ªÅ:** {ai_data['category']}\n**T·ª´ kh√≥a:** {ai_data['keywords']}\n\n{ai_data['summary']}"
                
                cursor.execute(sql, (ai_data['title'], f"/{rel_path}".replace("\\", "/"), total_pages, final_summary))
                conn.commit()
                
                print(f"‚úÖ ƒê√£ th√™m: {ai_data['title']} ({ai_data['category']})")
                count += 1
                
                # Ngh·ªâ 2s ƒë·ªÉ tr√°nh rate limit c·ªßa Google
                time.sleep(2)
            else:
                print("‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c d·ªØ li·ªáu t·ª´ AI.")

    print(f"\nüéâ HO√ÄN T·∫§T! ƒê√£ import th√†nh c√¥ng {count} t√†i li·ªáu.")
    conn.close()

if __name__ == "__main__":
    main()